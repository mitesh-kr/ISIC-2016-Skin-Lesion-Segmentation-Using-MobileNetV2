# -*- coding: utf-8 -*-
"""M23MAC004.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i6P4JrgFBwt0FwYUfz1kvjIUb-KO_Uz3
"""

from PIL import Image
import os
import torch
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import torchvision
from torch.utils.data import DataLoader
from torch.utils.data import TensorDataset
import torchvision.models as models
from torchsummary import summary
import torch.nn as nn
import torch.nn.functional as F
from google.colab import drive
import matplotlib.pyplot as plt
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

drive.mount('/content/drive')
train_path = ("/content/drive/MyDrive/Assignment/Dataset/train")
train_masks_path = ("/content/drive/MyDrive/Assignment/Dataset/train_masks")
val_path = ("/content/drive/MyDrive/Assignment/Dataset/test")
val_masks_path = ("/content/drive/MyDrive/Assignment/Dataset/test_masks")

# Function to load image and its mark and  resize it into 128 x 128 and convert into tensor
def load_images(image_folder_path, mask_folder_path):
    image_file_name = sorted(os.listdir(image_folder_path))
    mask_file_name = sorted(os.listdir(mask_folder_path))

    images = torch.tensor([])
    masks = torch.tensor([])
    for image_name, mask_name in zip(image_file_name, mask_file_name):

        if image_name.split('.')[0] == mask_name.split('.')[0]:
            image_path = os.path.join(image_folder_path, image_name)
            mask_path = os.path.join(mask_folder_path,mask_name)

            image = Image.open(image_path)
            image = transforms.Resize((128, 128))(image)
            image = transforms.ToTensor()(image)
            images = torch.cat((images,image.unsqueeze(0)),dim = 0)

            mask = Image.open(mask_path)
            mask = transforms.Resize((128, 128))(mask)
            mask = transforms.ToTensor()(mask)
            masks = torch.cat((masks,mask.unsqueeze(0)),dim = 0)

    return image_file_name, images, masks

def IoU(pred_mask, true_mask):
    # Flatten the masks along the batch dimension
    true_mask_flat = true_mask.view(true_mask.size(0), -1)
    pred_mask_flat = pred_mask.view(pred_mask.size(0), -1)
    intersect = torch.sum(pred_mask_flat * true_mask_flat, dim = 1)
    union = torch.sum(pred_mask_flat, dim = 1) + torch.sum(true_mask_flat, dim=1) - intersect
    iou = intersect / union
    mean_iou = torch.mean(iou)
    return mean_iou

# Function for calculate dice score of true and predicted mask
def dice(pred_mask, true_mask):
    pred_mask_flat = pred_mask.view(pred_mask.size(0), -1)
    true_mask_flat = true_mask.view(true_mask.size(0), -1)
    intersect = torch.sum(pred_mask_flat * true_mask_flat, dim=1)
    fsum = torch.sum(pred_mask_flat, dim=1)
    ssum = torch.sum(true_mask_flat, dim=1)
    dice = (2 * intersect) / (fsum + ssum + 1e-7)
    mean_dice = torch.mean(dice)
    return mean_dice.item()

# Function to plot graph of losses of training and validation together
def plot(epoch_values1, epoch_values2, ylabel1, ylabel2, title):
    epochs = range(1, len(epoch_values1) + 1)
    plt.figure(figsize=(10, 5))
    plt.plot(epochs, epoch_values1, marker='o', linestyle='-', color='r', label=ylabel1)
    plt.plot(epochs, epoch_values2, marker='o', linestyle='-', color='g', label=ylabel2)
    plt.xlabel('Epoch')
    plt.ylabel('Values')
    plt.title(title)
    plt.grid(True)
    plt.legend()
    plt.show()

def count_parameters(model):
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)
    return trainable_params, non_trainable_params

# Extract train image and masks using load function
_ , train_images, train_masks = load_images(train_path, train_masks_path)
train_dataset = TensorDataset(train_images, train_masks)
train_dataloader = DataLoader(train_dataset, batch_size = 45, shuffle = True, num_workers = 2)
# Load validation data
val_image_name, val_images, val_masks = load_images(val_path, val_masks_path)

# Model Architecture
class SegmentationModel(nn.Module):
    def __init__(self):
        super(SegmentationModel, self).__init__()

        self.encoder = models.mobilenet_v2(pretrained=True).features

        self.decoder = nn.Sequential(

            nn.Sequential(
                nn.ConvTranspose2d(1280, 320, kernel_size = 1, stride = 1),
                nn.BatchNorm2d(320),
                nn.ReLU(),
                nn.Dropout2d(0.4)),

            nn.Sequential(
                nn.ConvTranspose2d(320*2 , 160, kernel_size=3, stride=1, padding=1),
                nn.BatchNorm2d(160),
                nn.ReLU(),
                nn.Dropout2d(0.4)),

            nn.Sequential(
                nn.ConvTranspose2d(160*2, 96, kernel_size = 2 ,stride = 2),
                nn.BatchNorm2d(96),
                nn.ReLU(),
                nn.Dropout2d(0.4)),

            nn.Sequential(
                nn.ConvTranspose2d(96*2 , 64, kernel_size = 3, stride = 1 ,padding = 1),
                nn.BatchNorm2d(64),
                nn.ReLU(),
                nn.Dropout2d(0.4)),

            nn.Sequential(
                nn.ConvTranspose2d(64*2 ,32, kernel_size = 2, stride = 2),
                nn.BatchNorm2d(32),
                nn.ReLU(),
                nn.Dropout2d(0.4)),

            nn.Sequential(
                nn.ConvTranspose2d(32*2 ,24, kernel_size = 2, stride = 2),
                nn.BatchNorm2d(24),
                nn.ReLU(),
                nn.Dropout2d(0.4)),

            nn.Sequential(
                nn.ConvTranspose2d(24*2 ,16, kernel_size = 2, stride = 2),
                nn.BatchNorm2d(16),
                nn.ReLU(),
                nn.Dropout2d(0.4)),

            nn.Sequential(
                nn.ConvTranspose2d(16*2 ,32, kernel_size = 1, stride = 1),
                nn.BatchNorm2d(32),
                nn.ReLU(),
                nn.Dropout2d(0.4)),

            nn.Sequential(
                nn.ConvTranspose2d(32*2 ,3, kernel_size = 2, stride = 2),
                nn.BatchNorm2d(3),
                nn.ReLU(),
                nn.Dropout2d(0.4)),

            nn.Sequential(
                nn.ConvTranspose2d(3*2 ,1, kernel_size = 1, stride = 1),
                nn.BatchNorm2d(1),
                nn.Sigmoid()))

    def forward(self, x):
        r0 = x.clone()

        x = self.encoder[0](x)  # 32,128,128
        r1 = x.clone()

        x = self.encoder[1](x)     # 16,64,64
        r2 = x.clone()

        x = self.encoder[2](x)     # 24,32,32
        x = self.encoder[3](x)    # 24,32,32
        r3 = x.clone()

        x = self.encoder[4](x)    # 32,16,16
        r4 = x.clone()
        x = self.encoder[5](x)     # 32,16,16
        x = self.encoder[6](x)     # 32,16,16

        x = self.encoder[7](x)     # 64,8,8
        r5 = x.clone()
        x = self.encoder[8](x)      # 64,8,8
        x = self.encoder[9](x)     # 64,8,8
        x = self.encoder[10](x)   # 64,8,8

        x = self.encoder[11](x)   # 96,8,8
        r6 = x.clone()
        x = self.encoder[12](x)   # 96,8,8
        x = self.encoder[13](x)   # 96,8,8

        x = self.encoder[14](x)   #160,4,4
        r7 = x.clone()
        x = self.encoder[15](x)    #160,4,4
        x = self.encoder[16](x)   #160,4,4

        x = self.encoder[17](x)   # 320,4,4
        r8 = x.clone()
        x = self.encoder[18](x)    #1280,4,4


        x = self.decoder[0](x)          #[-1, 320, 4, 4]
        x = torch.cat((x, r8), dim=1)
        x = self.decoder[1](x)         # [-1, 160, 4, 4]
        x = torch.cat((x, r7), dim=1)
        x = self.decoder[2](x)          #[-1, 96, 8, 8]
        x = torch.cat((x, r6), dim=1)

        x = self.decoder[3](x)          # [-1, 64, 8, 8]
        x = torch.cat((x, r5), dim=1)

        x = self.decoder[4](x)          #[-1, 32, 16, 16]
        x = torch.cat((x, r4), dim=1)

        x = self.decoder[5](x)          #[-1, 24, 32, 32]
        x = torch.cat((x, r3), dim=1)

        x = self.decoder[6](x)           #[-1, 16, 64, 64]
        x = torch.cat((x, r2), dim=1)

        x = self.decoder[7](x)           #[-1, 32, 64, 64]
        x = torch.cat((x, r1), dim=1)

        x = self.decoder[8](x)           #[-1, 1, 128, 128]
        x = torch.cat((x, r0), dim=1)

        x = self.decoder[9](x)
        return x

#MODEL 1
print('MODEL 1')
model_1 = SegmentationModel().to(device)
loss_function = nn.BCELoss()
encoder_lr = 0
decoder_lr = 0.01

for param in model_1.encoder.parameters():
    param.requires_grad = False

# Define optimizer with separate learning rates
optimizer = torch.optim.Adam([
    {'params': model_1.encoder.parameters(), 'lr': encoder_lr},
    {'params': model_1.decoder.parameters(), 'lr': decoder_lr}
])

num_epochs = 50

train_loss_list_1 = []
val_loss_list_1 = []
val_iou_list_1 = []
val_dice_score_list_1 = []

for epoch in range(num_epochs):
    train_loss = 0

    model_1.train()
    for X_batch, y_batch in train_dataloader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        y_pred = model_1(X_batch)
        loss = loss_function(y_pred, y_batch)
        train_loss += loss.item()

        # Backward pass and optimization step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    avg_train_loss = train_loss / len(train_dataloader)
    train_loss_list_1.append(avg_train_loss)

    model_1.eval()
    with torch.no_grad():
        X_val, y_val = val_images, val_masks
        X_val, y_val = X_val.to(device), y_val.to(device)
        y_pred_val_1 = model_1(X_val)
        val_loss = loss_function(y_pred_val_1, y_val).item()
        val_iou = IoU(y_pred_val_1, y_val)
        val_dice_score = dice(y_pred_val_1, y_val)
        print(f'Epoch {epoch+1}  | Train Loss: {avg_train_loss:.4f}  | Val Loss: {val_loss:.4f}  | Val IoU: {val_iou:.4f}  | Val Dice Score:{val_dice_score:.4f}')

    val_loss_list_1.append(val_loss)
    val_iou_list_1.append(val_iou)
    val_dice_score_list_1.append(val_dice_score)

plot(epoch_values1 = train_loss_list_1, epoch_values2 = val_loss_list_1 , ylabel1 = 'Train Loss', ylabel2 = 'Val Loss', title = 'Train Loss Vs Val Loss')
trainable_params, non_trainable_params = count_parameters(model_1)
print(f"Number of trainable parameters: {trainable_params}")
print(f"Number of non-trainable parameters: {non_trainable_params}")

#MODEL 2
print('MODEL 2')
model_2 = SegmentationModel().to(device)
loss_function = nn.BCELoss()
encoder_lr = 0.0001
decoder_lr = 0.01

# Define optimizer with separate learning rates
optimizer = torch.optim.Adam([
    {'params': model_2.encoder.parameters(), 'lr': encoder_lr},
    {'params': model_2.decoder.parameters(), 'lr': decoder_lr}
])

num_epochs = 50

train_loss_list_2 = []
val_loss_list_2 = []
val_iou_list_2 = []
val_dice_score_list_2 = []

for epoch in range(num_epochs):
    train_loss = 0

    model_2.train()
    for X_batch, y_batch in train_dataloader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        y_pred = model_2(X_batch)
        loss = loss_function(y_pred, y_batch)
        train_loss += loss.item()

        # Backward pass and optimization step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    avg_train_loss = train_loss / len(train_dataloader)
    train_loss_list_2.append(avg_train_loss)

    model_2.eval()
    with torch.no_grad():
        X_val, y_val = val_images, val_masks
        X_val, y_val = X_val.to(device), y_val.to(device)
        y_pred_val_2 = model_2(X_val)
        val_loss = loss_function(y_pred_val_2, y_val).item()
        val_iou = IoU(y_pred_val_2, y_val)
        val_dice_score = dice(y_pred_val_2, y_val)
        print(f'Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Val IoU: {val_iou:.4f} | Val Dice Score:{val_dice_score:.4f}')

    val_loss_list_2.append(val_loss)
    val_iou_list_2.append(val_iou)
    val_dice_score_list_2.append(val_dice_score)

plot(epoch_values1 = train_loss_list_2, epoch_values2 = val_loss_list_2 , ylabel1 = 'Train Loss', ylabel2 = 'Val Loss', title = 'Train Loss Vs Val Loss')
trainable_params, non_trainable_params = count_parameters(model_2)
print(f"Number of trainable parameters: {trainable_params}")
print(f"Number of non-trainable parameters: {non_trainable_params}")

#Plot to show loss of model 1 vs model 2
plot(epoch_values1 = train_loss_list_1, epoch_values2 = train_loss_list_2 , ylabel1 = 'Model_1', ylabel2 = 'Model_2', title = 'Train Loss Model_1 Vs Model_2')
plot(epoch_values1 = val_loss_list_1, epoch_values2 = val_loss_list_2 , ylabel1 = 'Model_1', ylabel2 = 'Model_2', title = 'Val Loss Model_1 Vs Model_2')

# Function of show images and predicted masks
def showimage(idx):
    image_tensors = [val_images[idx], val_masks[idx], y_pred_val_1[idx], y_pred_val_1[idx], y_pred_val_2[idx], y_pred_val_2[idx]]
    thresholds = [None, None, None, 0.5, None, 0.5]
    num_images = len(image_tensors)
    plt.figure(figsize=(15, 3))

    for i in range(num_images):
        plt.subplot(1, num_images, i+1)
        if thresholds is not None:
            threshold = thresholds[i]
        else:
            threshold = None
        image_tensor = image_tensors[i]
        if threshold is not None:
            image_tensor = (image_tensor > threshold).float()

        image_pil = transforms.ToPILImage()(image_tensor)
        plt.imshow(image_pil, cmap='gray')
        plt.axis('off')
    plt.subplots_adjust(wspace=0.9)
    plt.tight_layout()
    plt.show()

print("                                 Ground Truth Mask","              model_1 mask" , "         model_1 mask with thresholds" , "       model_2 mask", "      model_2 mask with thresholds")
for i in [3,5,11,17,20,22,24,25,27,30,348,0]:
    name = val_image_name[i]
    print("    ",  name)
    showimage(i)

